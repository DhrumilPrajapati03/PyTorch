{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Detection Using U-Net with ResNet34 Backbone\n",
    "# Author: Dhrumil Prajapati\n",
    "# Description: Satellite image change detection using semantic segmentation with U-Net architecture\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from albumentations import (\n",
    "    Compose, RandomRotate90, RandomCrop, HorizontalFlip, VerticalFlip, Resize,\n",
    "    RandomBrightnessContrast, HueSaturationValue, Normalize, ShiftScaleRotate,\n",
    "    RandomGamma, ElasticTransform, GaussNoise, OneOf\n",
    ")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "data_dir = \"/content/drive/MyDrive/dataset_onera\"\n",
    "model_save_path = \"best_model.pth\"\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    \"img_size\": 256,\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 50,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"encoder\": \"resnet34\",\n",
    "    \"encoder_weights\": \"imagenet\",\n",
    "    \"classes\": 1,\n",
    "    \"activation\": None,  # We'll apply sigmoid manually for more flexibility\n",
    "    \"val_size\": 0.1,\n",
    "    \"test_size\": 0.1\n",
    "}\n",
    "\n",
    "# Data preparation\n",
    "def read_img(path):\n",
    "    \"\"\"Read image from path and convert to RGB format\"\"\"\n",
    "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def read_mask(path):\n",
    "    \"\"\"Read mask from path and normalize\"\"\"\n",
    "    mask = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    mask = mask / 255.0\n",
    "    return mask\n",
    "\n",
    "def get_data_paths():\n",
    "    \"\"\"Get all image and mask paths from the dataset\"\"\"\n",
    "    img1_paths = sorted(glob(os.path.join(data_dir, \"imgs/im1/*.png\")))\n",
    "    img2_paths = sorted(glob(os.path.join(data_dir, \"imgs/im2/*.png\")))\n",
    "    mask_paths = sorted(glob(os.path.join(data_dir, \"gt/*.png\")))\n",
    "    \n",
    "    # Create DataFrame for easier data handling\n",
    "    df = pd.DataFrame({\n",
    "        \"img1_path\": img1_paths,\n",
    "        \"img2_path\": img2_paths,\n",
    "        \"mask_path\": mask_paths\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Augmentations\n",
    "def get_training_augmentation():\n",
    "    \"\"\"Get augmentation pipeline for training data\"\"\"\n",
    "    return Compose([\n",
    "        RandomRotate90(p=0.5),\n",
    "        HorizontalFlip(p=0.5),\n",
    "        VerticalFlip(p=0.5),\n",
    "        ShiftScaleRotate(scale_limit=0.2, rotate_limit=45, p=0.5),\n",
    "        RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "        OneOf([\n",
    "            ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03, p=0.5),\n",
    "            GaussNoise(var_limit=(10, 50), p=0.5),\n",
    "            RandomGamma(gamma_limit=(80, 120), p=0.5)\n",
    "        ], p=0.5),\n",
    "        Resize(height=config[\"img_size\"], width=config[\"img_size\"], always_apply=True),\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), always_apply=True)\n",
    "    ])\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Get augmentation pipeline for validation data\"\"\"\n",
    "    return Compose([\n",
    "        Resize(height=config[\"img_size\"], width=config[\"img_size\"], always_apply=True),\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), always_apply=True)\n",
    "    ])\n",
    "\n",
    "# Dataset\n",
    "class ChangeDetectionDataset(Dataset):\n",
    "    def __init__(self, df, augmentation=None):\n",
    "        self.df = df\n",
    "        self.augmentation = augmentation\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img1_path = self.df.iloc[idx].img1_path\n",
    "        img2_path = self.df.iloc[idx].img2_path\n",
    "        mask_path = self.df.iloc[idx].mask_path\n",
    "        \n",
    "        img1 = read_img(img1_path)\n",
    "        img2 = read_img(img2_path)\n",
    "        mask = read_mask(mask_path)\n",
    "        \n",
    "        # Stack images to create a 6-channel input\n",
    "        img = np.concatenate([img1, img2], axis=2)\n",
    "        \n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=img, mask=mask)\n",
    "            img, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # Reshape mask for model input\n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "        \n",
    "        return torch.from_numpy(img.transpose(2, 0, 1)).float(), torch.from_numpy(mask).float()\n",
    "\n",
    "# Model\n",
    "def build_model():\n",
    "    \"\"\"Build U-Net model with ResNet34 backbone\"\"\"\n",
    "    model = smp.Unet(\n",
    "        encoder_name=config[\"encoder\"],\n",
    "        encoder_weights=config[\"encoder_weights\"],\n",
    "        in_channels=6,  # 3 channels from img1 + 3 channels from img2\n",
    "        classes=config[\"classes\"],\n",
    "        activation=config[\"activation\"]\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "# Combined Loss Function\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, weights=(0.5, 0.5)):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice_loss = smp.losses.DiceLoss(mode='binary')\n",
    "        self.weights = weights\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        bce_loss = self.bce(y_pred, y_true)\n",
    "        dice_loss = self.dice_loss(torch.sigmoid(y_pred), y_true)\n",
    "        return self.weights[0] * bce_loss + self.weights[1] * dice_loss\n",
    "\n",
    "# Metrics\n",
    "def iou_score(y_pred, y_true, threshold=0.5):\n",
    "    \"\"\"Calculate IoU score\"\"\"\n",
    "    y_pred = (torch.sigmoid(y_pred) > threshold).float()\n",
    "    intersection = (y_pred * y_true).sum()\n",
    "    union = y_pred.sum() + y_true.sum() - intersection\n",
    "    return (intersection + 1e-7) / (union + 1e-7)\n",
    "\n",
    "def f1_score(y_pred, y_true, threshold=0.5):\n",
    "    \"\"\"Calculate F1 score\"\"\"\n",
    "    y_pred = (torch.sigmoid(y_pred) > threshold).float()\n",
    "    tp = (y_true * y_pred).sum()\n",
    "    fp = ((1 - y_true) * y_pred).sum()\n",
    "    fn = (y_true * (1 - y_pred)).sum()\n",
    "    precision = tp / (tp + fp + 1e-7)\n",
    "    recall = tp / (tp + fn + 1e-7)\n",
    "    return 2 * precision * recall / (precision + recall + 1e-7)\n",
    "\n",
    "# Training functions\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_iou = 0\n",
    "    epoch_f1 = 0\n",
    "    \n",
    "    with tqdm(loader, desc=\"Training\", leave=False) as pbar:\n",
    "        for images, masks in pbar:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Metrics\n",
    "            iou = iou_score(outputs, masks)\n",
    "            f1 = f1_score(outputs, masks)\n",
    "            \n",
    "            # Update progress\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_iou += iou.item()\n",
    "            epoch_f1 += f1.item()\n",
    "            pbar.set_postfix(loss=loss.item(), iou=iou.item(), f1=f1.item())\n",
    "    \n",
    "    # Return average metrics\n",
    "    return epoch_loss / len(loader), epoch_iou / len(loader), epoch_f1 / len(loader)\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_iou = 0\n",
    "    epoch_f1 = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(loader, desc=\"Validation\", leave=False) as pbar:\n",
    "            for images, masks in pbar:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                \n",
    "                # Metrics\n",
    "                iou = iou_score(outputs, masks)\n",
    "                f1 = f1_score(outputs, masks)\n",
    "                \n",
    "                # Update progress\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_iou += iou.item()\n",
    "                epoch_f1 += f1.item()\n",
    "                pbar.set_postfix(loss=loss.item(), iou=iou.item(), f1=f1.item())\n",
    "    \n",
    "    # Return average metrics\n",
    "    return epoch_loss / len(loader), epoch_iou / len(loader), epoch_f1 / len(loader)\n",
    "\n",
    "# Visualization\n",
    "def visualize_predictions(model, test_loader, device, num_samples=5):\n",
    "    \"\"\"Visualize model predictions on test data\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    images, masks, predictions = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (img, mask) in enumerate(test_loader):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "                \n",
    "            img = img.to(device)\n",
    "            pred = model(img)\n",
    "            pred = torch.sigmoid(pred).cpu().numpy()\n",
    "            \n",
    "            # Store original images, masks and predictions\n",
    "            img1 = img[0, :3].cpu().numpy().transpose(1, 2, 0)\n",
    "            img2 = img[0, 3:].cpu().numpy().transpose(1, 2, 0)\n",
    "            mask = mask[0, 0].cpu().numpy()\n",
    "            pred = pred[0, 0]\n",
    "            \n",
    "            # Denormalize images\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            img1 = std * img1 + mean\n",
    "            img2 = std * img2 + mean\n",
    "            img1 = np.clip(img1, 0, 1)\n",
    "            img2 = np.clip(img2, 0, 1)\n",
    "            \n",
    "            images.append((img1, img2))\n",
    "            masks.append(mask)\n",
    "            predictions.append(pred)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(20, 4*num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Image 1\n",
    "        axes[i, 0].imshow(images[i][0])\n",
    "        axes[i, 0].set_title('Image 1')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Image 2\n",
    "        axes[i, 1].imshow(images[i][1])\n",
    "        axes[i, 1].set_title('Image 2')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Ground Truth\n",
    "        axes[i, 2].imshow(masks[i], cmap='gray')\n",
    "        axes[i, 2].set_title('Ground Truth')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        # Prediction\n",
    "        axes[i, 3].imshow((predictions[i] > 0.5).astype(np.uint8), cmap='gray')\n",
    "        axes[i, 3].set_title('Prediction')\n",
    "        axes[i, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Main training loop\n",
    "def train_model():\n",
    "    # Prepare data\n",
    "    print(\"Preparing data...\")\n",
    "    df = get_data_paths()\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_df, temp_df = train_test_split(df, test_size=config[\"val_size\"]+config[\"test_size\"], random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=config[\"test_size\"]/(config[\"val_size\"]+config[\"test_size\"]), random_state=42)\n",
    "    \n",
    "    print(f\"Train samples: {len(train_df)}\")\n",
    "    print(f\"Validation samples: {len(val_df)}\")\n",
    "    print(f\"Test samples: {len(test_df)}\")\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = ChangeDetectionDataset(train_df, augmentation=get_training_augmentation())\n",
    "    val_dataset = ChangeDetectionDataset(val_df, augmentation=get_validation_augmentation())\n",
    "    test_dataset = ChangeDetectionDataset(test_df, augmentation=get_validation_augmentation())\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Build model\n",
    "    print(\"Building model...\")\n",
    "    model = build_model()\n",
    "    print(f\"Model: U-Net with {config['encoder']} backbone\")\n",
    "    \n",
    "    # Set up training\n",
    "    criterion = CombinedLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"Starting training for {config['epochs']} epochs...\")\n",
    "    history = {\n",
    "        'train_loss': [], 'train_iou': [], 'train_f1': [],\n",
    "        'val_loss': [], 'val_iou': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    best_iou = 0\n",
    "    \n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        print(f\"Epoch {epoch+1}/{config['epochs']}\")\n",
    "        \n",
    "        # Train and validate\n",
    "        train_loss, train_iou, train_f1 = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_iou, val_f1 = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_iou)\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_iou'].append(train_iou)\n",
    "        history['train_f1'].append(train_f1)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_iou'].append(val_iou)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Train Loss: {train_loss:.4f}, IoU: {train_iou:.4f}, F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, IoU: {val_iou:.4f}, F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_iou > best_iou:\n",
    "            best_iou = val_iou\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Saved best model with IoU: {best_iou:.4f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_loss, test_iou, test_f1 = validate_epoch(model, test_loader, criterion, device)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, IoU: {test_iou:.4f}, F1: {test_f1:.4f}\")\n",
    "    \n",
    "    # Visualize results\n",
    "    fig = visualize_predictions(model, test_loader, device)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_iou'], label='Train IoU')\n",
    "    plt.plot(history['val_iou'], label='Val IoU')\n",
    "    plt.title('IoU Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return model, history, test_iou, test_f1\n",
    "\n",
    "# Run the training process\n",
    "if __name__ == \"__main__\":\n",
    "    model, history, test_iou, test_f1 = train_model()\n",
    "    print(\"Training completed!\")\n",
    "    print(f\"Final Test IoU: {test_iou:.4f}\")\n",
    "    print(f\"Final Test F1 Score: {test_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
